{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32cff3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updates to keyboard shortcuts â€¦ On Thursday, August 1, 2024, Drive keyboard shortcuts will be updated to give you first-letters navigation.Learn more\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import csv\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10b7f3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13f24d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "num_classes = 2\n",
    "max_length = 256\n",
    "batch_size = 16\n",
    "num_epochs = 3\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d013adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_data(data_file):\n",
    "    df = pd.read_csv(data_file)\n",
    "    texts = df['text'].tolist()\n",
    "    labels = df['generated'].tolist()  # Assuming 'label' column contains the labels directly\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d38fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4dd1269",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"C:/Users/HP VICTUS/Desktop/Book.csv\"\n",
    "texts, labels = load_imdb_data(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eee4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43265437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae663ec",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cc28030",
   "metadata": {},
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818a8a7f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "    def forward(self, input_ids, attention_mask):  # Correct indentation here\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04e6f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        # Include attention dropout in the configuration\n",
    "        config = BertConfig.from_pretrained(bert_model_name)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name, config=config)\n",
    "        # self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        # x = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6aaf68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29086e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77f55fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_examples = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        optimizer.zero_grad()  # Clear gradients before each optimization step\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Get model outputs, which are the logits in this case\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Calculate loss using the logits and actual labels\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "\n",
    "\n",
    "\n",
    "        # l2_reg_loss = sum(torch.norm(param) ** 2 for param in model.parameters())\n",
    "        # loss += 0.5 * 0.05 * l2_reg_loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted_labels = torch.max(logits, dim=1)\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "        total_examples += labels.size(0)\n",
    "        loss.backward()  # Backpropagate the error\n",
    "        optimizer.step()  # Update parameters\n",
    "        scheduler.step()  # Update learning rate\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(data_loader)}: Loss {loss.item()}\")\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    train_accuracy = correct_predictions / total_examples\n",
    "\n",
    "    return average_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6e57eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            # l2_reg_loss = sum(torch.norm(param) ** 2 for param in model.parameters())\n",
    "            # loss += 0.5 * 0.05 * l2_reg_loss\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "            \n",
    "    accuracy = accuracy_score(actual_labels, predictions)\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    return accuracy, average_loss,classification_report(actual_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22de7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text_source(text, model, tokenizer, device, max_length=128):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "    return \"AI-generated\" if preds.item() == 1 else \"Human-written\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d28df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f511e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "239492f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8c975d1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on :  cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on : \",device)\n",
    "model = BERTClassifier(bert_model_name, num_classes).to(device)\n",
    "#model.load_state_dict(torch.load(\"bert_classifier.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88952720",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP VICTUS\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learning_rate,weight_decay=0.05)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10c0fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1261e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_metric = float('-inf')  # Initialize best validation metric (can be accuracy or loss)\n",
    "patience = 3  # Number of epochs to wait for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94d14593",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Batch 0/23: Loss 0.68047696352005\n",
      "Validation Accuracy: 0.9430\n",
      "Training Accuracy: 0.9266\n",
      "Training Loss: 0.3440\n",
      "Validation Loss: 0.2165\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        81\n",
      "           1       1.00      0.88      0.94        77\n",
      "\n",
      "    accuracy                           0.94       158\n",
      "   macro avg       0.95      0.94      0.94       158\n",
      "weighted avg       0.95      0.94      0.94       158\n",
      "\n",
      "Epoch 2/3\n",
      "Batch 0/23: Loss 0.12752029299736023\n",
      "Validation Accuracy: 0.9430\n",
      "Training Accuracy: 0.9592\n",
      "Training Loss: 0.1532\n",
      "Validation Loss: 0.1486\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        81\n",
      "           1       1.00      0.88      0.94        77\n",
      "\n",
      "    accuracy                           0.94       158\n",
      "   macro avg       0.95      0.94      0.94       158\n",
      "weighted avg       0.95      0.94      0.94       158\n",
      "\n",
      "Epoch 3/3\n",
      "Batch 0/23: Loss 0.05893878638744354\n",
      "Validation Accuracy: 0.9430\n",
      "Training Accuracy: 0.9674\n",
      "Training Loss: 0.0976\n",
      "Validation Loss: 0.1242\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        81\n",
      "           1       1.00      0.88      0.94        77\n",
      "\n",
      "    accuracy                           0.94       158\n",
      "   macro avg       0.95      0.94      0.94       158\n",
      "weighted avg       0.95      0.94      0.94       158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss,train_accuracy = train(model, train_dataloader, optimizer, scheduler, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracy, val_loss,report = evaluate(model, val_dataloader, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    print(report)\n",
    "    # if val_accuracy > best_val_metric:  # Replace 'val_accuracy' with loss if needed\n",
    "    #     best_val_metric = val_accuracy\n",
    "    #     patience_counter = 0  # Reset patience counter\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "\n",
    "    # if patience_counter >= patience:\n",
    "    #     print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "282b9df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and configuration have been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Ensure the model directory exists\n",
    "model_directory = \"model\"\n",
    "os.makedirs(model_directory, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Save the model weights to 'pytorch_model.bin'\n",
    "torch.save(model.state_dict(), os.path.join(model_directory, \"pytorch_model.bin\"))\n",
    "\n",
    "# Optionally save the configuration as a JSON file\n",
    "config = {\n",
    "    \"num_labels\": 2,  # Example: number of output labels\n",
    "    \"model_name\": \"bert-base-uncased\",  # Base model used\n",
    "}\n",
    "\n",
    "# Save configuration to 'config.json'\n",
    "with open(os.path.join(model_directory, \"config.json\"), \"w\") as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "print(\"Model and configuration have been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3729d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP VICTUS\\AppData\\Local\\Temp\\ipykernel_12400\\698801374.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model/bert_classifier.pth\", map_location=\"cpu\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERTClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        # Include attention dropout in the configuration\n",
    "        config = BertConfig.from_pretrained(bert_model_name)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name, config=config)\n",
    "        # self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        # x = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "with open(\"model/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Reconstruct the model\n",
    "model = BERTClassifier(config[\"model_name\"], config[\"num_labels\"])\n",
    "\n",
    "# Load the saved weights\n",
    "model.load_state_dict(torch.load(\"model/bert_classifier.pth\", map_location=\"cpu\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7de2b364",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "essay=\"dogs ar very friendly animals ,they are used to gaurd the house and can safegaurd owners belonging\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e6e8a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human-written\n"
     ]
    }
   ],
   "source": [
    "result=predict_text_source(essay, model, tokenizer, device)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c168ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, num_epochs + 1)\n",
    "# Create traces for training and validation loss\n",
    "trace1 = go.Scatter(\n",
    "    x=list(epochs),\n",
    "    y=train_losses,\n",
    "    mode='lines+markers',\n",
    "    name='Training Loss',\n",
    "    marker=dict(color='blue')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c6470",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace2 = go.Scatter(\n",
    "    x=list(epochs),\n",
    "    y=val_losses,\n",
    "    mode='lines+markers',\n",
    "    name='Validation Loss',\n",
    "    marker=dict(color='red')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef91261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create traces for training and validation accuracy\n",
    "trace3 = go.Scatter(\n",
    "    x=list(epochs),\n",
    "    y=train_accuracies,\n",
    "    mode='lines+markers',\n",
    "    name='Training Accuracy',\n",
    "    marker=dict(color='purple')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e982e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "trace4 = go.Scatter(\n",
    "    x=list(epochs),\n",
    "    y=val_accuracies,\n",
    "    mode='lines+markers',\n",
    "    name='Validation Accuracy',\n",
    "    marker=dict(color='green')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb8c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure and add traces for loss\n",
    "fig = go.Figure()\n",
    "fig.add_trace(trace1)\n",
    "fig.add_trace(trace2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423df792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set layout for loss plot\n",
    "fig.update_layout(\n",
    "    title='Training and Validation Loss',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Loss',\n",
    "    legend_title='Legend',\n",
    "    width=600,  # adjust size as needed\n",
    "    height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cdb299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0904c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure for accuracy\n",
    "fig2_accuracy = go.Figure()\n",
    "fig2_accuracy.add_trace(trace3)\n",
    "fig2_accuracy.add_trace(trace4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a62c1c6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set layout for accuracy plot\n",
    "fig2_accuracy.update_layout(\n",
    "    title='Training and Validation Accuracy',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Accuracy',\n",
    "    legend_title='Legend',\n",
    "    width=600,  # adjust size as needed\n",
    "    height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ca307",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Show the figure\n",
    "fig2_accuracy.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
